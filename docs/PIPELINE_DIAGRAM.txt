```
YOLOv8n-RefDet Training Pipeline Architecture
==============================================

┌─────────────────────────────────────────────────────────────────────────────┐
│                          DATA LOADING PIPELINE                               │
└─────────────────────────────────────────────────────────────────────────────┘

┌──────────────────┐
│ annotations.json │ ──> Parse video_id, frame_idx, bboxes
└──────────────────┘

┌──────────────────────┐
│ drone_video.mp4      │ ──> Extract frame at frame_idx
└──────────────────────┘     (with caching)

┌──────────────────────┐
│ object_images/       │ ──> Load 3 support images
│  ├─ img_1.jpg       │
│  ├─ img_2.jpg       │
│  └─ img_3.jpg       │
└──────────────────────┘

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                      EPISODIC BATCH SAMPLER                                  │
│                                                                              │
│  Episode = N-way K-shot Q-query                                             │
│                                                                              │
│  Sample N classes (e.g., N=2):                                              │
│    Class 0: Backpack_0                                                      │
│    Class 1: Laptop_1                                                        │
│                                                                              │
│  For each class, sample Q queries (e.g., Q=4):                              │
│    Backpack_0: [frame_3483, frame_3500, frame_3520, frame_3540]            │
│    Laptop_1:   [frame_1200, frame_1220, frame_1240, frame_1260]            │
│                                                                              │
│  K support images per class (K=3, fixed by dataset)                         │
│                                                                              │
│  Total batch size = N × Q = 2 × 4 = 8 query images                          │
└─────────────────────────────────────────────────────────────────────────────┘

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                         AUGMENTATION STAGE                                   │
└─────────────────────────────────────────────────────────────────────────────┘

Query Path (Drone Frames):                Support Path (Reference):
┌──────────────────────────┐             ┌──────────────────────────┐
│ • Ultralytics Mosaic     │             │ • Weak Augmentation      │
│ • Ultralytics MixUp      │             │ • Horizontal Flip        │
│ • AlbumentationsX:       │             │ • Random Crop (0.85-1.0) │
│   - Color jitter         │             │ • Hue/Sat/Val (+/-10)    │
│   - Geometric transforms │             │ • Target: 518×518        │
│   - Blur & noise         │             │ • Preserve semantics     │
│   - Random erasing       │             │   for DINOv2             │
│ • Target: 640×640        │             └──────────────────────────┘
└──────────────────────────┘

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                           MODEL FORWARD PASS                                 │
└─────────────────────────────────────────────────────────────────────────────┘

Support Images (N×K, 3, 518, 518)    Query Images (B, 3, 640, 640)
         ↓                                       ↓
┌─────────────────────┐              ┌─────────────────────┐
│  DINOv2 Encoder     │              │  YOLOv8n Backbone   │
│  (ViT-S/14)         │              │  (CSPDarknet)       │
│  • Frozen/slow LR   │              │  • Pretrained       │
│  • Extract features │              │  • Multi-scale out  │
│  • 5.7M params      │              │  • 3.2M params      │
└─────────────────────┘              └─────────────────────┘
         ↓                                       ↓
  [128, 256, 512]                         [64, 128, 256]
  Multi-scale features                    P3, P4, P5 features
         ↓                                       ↓
         └────────────────┬──────────────────────┘
                          ↓
                 ┌─────────────────────┐
                 │   SCS Fusion        │
                 │   • Cross-scale     │
                 │   • Attention-based │
                 │   • 1.2M params     │
                 └─────────────────────┘
                          ↓
                 [256, 512, 512]
                 Fused features
                          ↓
                 ┌─────────────────────┐
                 │  Dual Detection Head│
                 │  • Base + Novel     │
                 │  • Prototype match  │
                 │  • 0.5M params      │
                 └─────────────────────┘
                          ↓
         ┌────────────────┴────────────────┐
         ↓                                  ↓
    Predictions                        Features
    • Bboxes (N, 4)                   • Query features
    • Scores (N,)                     • Support prototypes
    • Classes (N,)                    • For contrastive loss
    • DFL dist (N, 68)

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                          LOSS COMPUTATION                                    │
└─────────────────────────────────────────────────────────────────────────────┘

Step 1: Match Predictions to Targets
┌────────────────────────────────────────┐
│ • Compute IoU between pred and GT      │
│ • Assign predictions to targets        │
│ • Extract matched pairs                │
└────────────────────────────────────────┘

Step 2: Compute Detection Losses
┌─────────────────────────────────────────────────────────────────┐
│ • WIoU (weight=7.5):   Bbox regression                          │
│ • BCE (weight=0.5):    Classification                           │
│ • DFL (weight=1.5):    Distribution focal loss                  │
└─────────────────────────────────────────────────────────────────┘

Step 3: Compute Contrastive Losses (Stage 2+)
┌─────────────────────────────────────────────────────────────────┐
│ • SupCon (weight=1.0→0.5):  Supervised contrastive learning     │
│ • CPE (weight=0.5→0.3):     Contrastive proposal encoding       │
└─────────────────────────────────────────────────────────────────┘

Step 4: Compute Triplet Loss (Stage 3)
┌─────────────────────────────────────────────────────────────────┐
│ • Triplet (weight=0.2):  Prevent catastrophic forgetting        │
└─────────────────────────────────────────────────────────────────┘

Step 5: Total Loss
┌─────────────────────────────────────────────────────────────────┐
│ Total = Σ (weight_i × loss_i)                                   │
└─────────────────────────────────────────────────────────────────┘

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                        OPTIMIZATION STEP                                     │
└─────────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────┐
│ 1. Backward Pass (with AMP)            │
│    • Mixed precision for speed         │
│    • GradScaler for stability          │
└────────────────────────────────────────┘
                ↓
┌────────────────────────────────────────┐
│ 2. Gradient Accumulation               │
│    • Accumulate over N steps           │
│    • Effective larger batch size       │
└────────────────────────────────────────┘
                ↓
┌────────────────────────────────────────┐
│ 3. Optimizer Step (AdamW)              │
│    • Layerwise learning rates:         │
│      - DINOv2: 1e-5                    │
│      - YOLOv8: 1e-4                    │
│      - Fusion: 2e-4                    │
│      - Head: 2e-4                      │
└────────────────────────────────────────┘
                ↓
┌────────────────────────────────────────┐
│ 4. Learning Rate Scheduling            │
│    • Cosine annealing                  │
│    • Warmup (optional)                 │
└────────────────────────────────────────┘

                ↓

┌─────────────────────────────────────────────────────────────────────────────┐
│                      LOGGING & CHECKPOINTING                                 │
└─────────────────────────────────────────────────────────────────────────────┘

Every N iterations:
  • Log total loss
  • Log loss components
  • Log learning rate

Every epoch:
  • Validate on test set
  • Compute mAP, precision, recall
  • Save checkpoint if best
  • Save latest checkpoint

End of training:
  • Save final model
  • Log best metrics


┌─────────────────────────────────────────────────────────────────────────────┐
│                         TRAINING STAGES                                      │
└─────────────────────────────────────────────────────────────────────────────┘

Stage 1: Base Pre-training (Optional)
┌──────────────────────────────────────────────────────────┐
│ • Train on COCO or base classes                          │
│ • Detection losses only (bbox + cls + dfl)               │
│ • Learn general object detection                         │
│ • ~50 epochs                                             │
└──────────────────────────────────────────────────────────┘

Stage 2: Few-Shot Meta-Learning (Main)
┌──────────────────────────────────────────────────────────┐
│ • Episodic training on novel objects                     │
│ • Full loss stack (detection + contrastive)              │
│ • Learn to adapt to new objects                          │
│ • ~100 epochs, 2-4 hours on RTX 3090                     │
└──────────────────────────────────────────────────────────┘

Stage 3: Fine-Tuning (Optional)
┌──────────────────────────────────────────────────────────┐
│ • Fine-tune on specific target objects                   │
│ • Reduced contrastive + triplet loss                     │
│ • Prevent forgetting base knowledge                      │
│ • ~30 epochs, 30-60 minutes                              │
└──────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│                         EVALUATION PIPELINE                                  │
└─────────────────────────────────────────────────────────────────────────────┘

For each episode:
  1. Sample N classes
  2. Load K support images per class
  3. Load Q query frames per class
  4. Encode support images → prototypes
  5. Cache prototypes in model
  6. Forward query frames → predictions
  7. Match predictions to ground truth
  8. Compute metrics (precision, recall, mAP)

Aggregate over all episodes:
  • Average precision/recall
  • mAP@0.5, mAP@0.5:0.95
  • Per-class performance


┌─────────────────────────────────────────────────────────────────────────────┐
│                         PERFORMANCE METRICS                                  │
└─────────────────────────────────────────────────────────────────────────────┘

Model Size:
  • Total parameters: ~10.4M (20.8% of 50M budget)
  • DINOv2: 5.7M
  • YOLOv8n: 3.2M
  • Fusion: 1.2M
  • Head: 0.5M

Training Speed (RTX 3090):
  • Stage 2: ~2-4 hours (100 epochs)
  • Stage 3: ~30-60 minutes (30 epochs)
  • Mixed precision: ~2x speedup

Inference Speed:
  • YOLOv8n base: ~2ms per frame
  • With support encoding: +1ms
  • Total: ~3ms per frame (333 FPS)
  • Real-time capable on Jetson Xavier NX

Detection Performance:
  • 1-shot mAP@0.5: 35-45%
  • 3-shot mAP@0.5: 50-60%
  • 5-shot mAP@0.5: 60-70%

Memory Usage:
  • Training: ~6-8GB GPU memory
  • Inference: ~2GB GPU memory
  • Frame cache: ~100 frames × 640×640×3 ≈ 500MB RAM
```
